{
    "arch": "GPU",
    ## Set a clock rate
    "clock_mhz": 700,
    "arch": "GPU",
    ## There are two memories layers of note in a GPU, the global memory and
    ## the per-multiprocessor local memory.  We model this a just a single
    ## instance of local memory (even though on a real GPU there is 1 per
    ## multiprocessor) since we are not concerned with the accurary of
    ## simulation yet.  We also ignore the L2 cache currently for similar
    ## reasons.
    "mem_units": {
        "GLOBAL": { "count": 1, "size_KiB": 3932160 }, {## max DDR: 3840 MiB ##}
        "LOCAL":  { "count": 1, "size_KiB": {{ LOCAL_MEM_KIB }} }
    },
    ## We add a transfer unit to move data to/from shared memory
    "tx_units": {
        "DMA": { "count": 1 }
    },
    ## We make only 1 execution unit for simplicity (again there is probably 1
    ## per multi-process in reality)
    "exec_units": {
        "GPU": { "count": 1, "ops_per_cycle": 512 }
    },
    ## We connect the various busses up, bytes_per_cycle is just a placeholder
    ## here since we are not focused on the simulation aspect right now
    "buses": [
        { "sources": ["GLOBAL[0]"], "sinks": ["DMA[0]"], "bytes_per_cycle": 64 },
        { "sources": ["DMA[0]"], "sinks": ["GLOBAL[0]"], "bytes_per_cycle": 64 },
        { "sources": ["LOCAL[0]"], "sinks": ["DMA[0]"], "bytes_per_cycle": 64 },
        { "sources": ["DMA[0]"], "sinks": ["LOCAL[0]"], "bytes_per_cycle": 64 },
        { "sources": ["LOCAL[0]"], "sinks": ["GPU[0]"], "bytes_per_cycle": 64 },
        { "sources": ["GPU[0]"], "sinks": ["LOCAL[0]"], "bytes_per_cycle": 64 },
        { "sources": ["GLOBAL[0]"], "sinks": ["GPU[0]"], "bytes_per_cycle": 64 },
        { "sources": ["GPU[0]"], "sinks": ["GLOBAL[0]"], "bytes_per_cycle": 64 }
    ],
    ## Define the stripe passes
    "passes": [
        ## First, we place all the initial buffer in global memory (DRAM)
        { "name": "loc_program", "locate_memory": { "reqs": ["program"], "loc": { "name": "GLOBAL" } } },
        { "name": "loc_main", "locate_memory": { "reqs": ["main"], "loc": { "name": "GLOBAL" } } },

        ## Eliminate the dead code first
        { "name": "dead_code_elimination", "dead_code_elimination": {"reqs": ["all"] } },

        ## Next, we find all the dense contraction blocks, and split them up
        ## into groups such that the inputs for the innermost loops fit into
        ## shared memory and the outputs fit into the register set of the each
        ## multiprocessor.
        {
            "name": "tile_contract",
            "autotile": {
                "reqs": ["contraction"],  ## Apply to only dense operations
                "outer_set": ["contract_outer", "kernel"],
                "inner_set": ["contract_inner"],
                "clear_outer": true,
                ## "acc_idxs": false,
                "only_po2": true,  ## Only consider PO2 sizes for speed
                "max_total_size": {{ LOCAL_MEM_KIB * 1024 }},  ## All inputs must fit in local memory
                "min_out_size": {{ NUM_THREADS }},
                ## Since all loads to/from global memory are across a wide bus, use that as the
                ## cache_width to optimize for contigous regions of DRAM for each inner block
                "cache_width": {{ CACHE_WIDTH }}
            }
        },

        ## Since we don't accumulate final outputs, we need to move any tiling over
        ## accumulation dimensions into an inner loop 
        {
            "name": "tile_middle",
            "autotile": {
                "reqs": ["contract_outer"],  ## Apply to only dense operations
                "inner_set": ["contract_middle"],
                "acc_idxs": false,
                ## With i/o costs zeroed out, and split factor set high, we basically
                ## should pick the largest tile that doesn't use accumuation indexes
                "input_cost": 0.0, 
                "output_cost": 0.0,
                "split_factor": -100.0,
                "only_po2": true,  ## Only consider PO2 sizes for speed
            }
        },

        ## Now we add explicit code to load inputs from DRAM to shared memory for each inner block
        ## Note, we don't cache outputs yet, since the next fusion step will split the outer loop into
        ## accumulation indexes and non-accumulation indexes
        { "name": "cache_input", "cache": { "reqs": ["contract_middle"], "dirs": [ "In" ], "mem_loc": { "name": "LOCAL" }, "xfer_loc": { "name": "DMA" } } },

        ## Next we fuse in any element-wise operations which operate on the output of contraction block
        { "name": "fuse_contract_eltwise", "fusion": { "a_reqs": ["contract_outer"], "b_reqs": ["eltwise"]  } },

        ## Clean up indexes to allow better fusion
        { "name": "prefuse", "prune_idxs": { "reqs": ["main"] } },

        ## Then we fuse multiple eltwise things
        { "name": "fuse_eltwise_eltwise", "fusion": { "parent_reqs": ["main"], "a_reqs": ["eltwise"], "b_reqs": ["eltwise"], "output_match": true } },

        ## Then we 'localize' buffers, which moves any temporaries the only are needed to hold the output
        ## of dense computations before they are used on elementwise operations into the interior of the fused blocks
        { "name": "localize_main", "localize": { "reqs": ["main"] } },

        ## Now we add the output caching, which will now have the same loop structure as the element-wise ops
        { "name": "cache_output", "cache": { "reqs": ["contract_outer"], "dirs": [ "Out" ], "mem_loc": { "name": "LOCAL" }, "xfer_loc": { "name": "DMA" } } },

        ## Clean up indexes to allow better fusion
        { "name": "prefuse", "prune_idxs": { "reqs": ["main"] } },

        ## We do an inner 'fusion' to move all 'per-output' operations into a single loop
        { "name": "fuse_inner", "fusion": { "parent_reqs": ["contract_outer"], "fused_set": ["cache"], "exclude": ["contract_middle"] } },

	## Now tile any remaining eltwise kernels
        {
            "name": "tile_eltwise",
            "autotile": {
                "reqs": ["eltwise", "kernel"],  ## Apply to only dense operations
                "outer_set": ["eltwise_outer", "kernel"],
                "inner_set": ["eltwise_inner"],
                "clear_outer": true,
                "only_po2": true,  ## Only consider PO2 sizes for speed
                "min_count": {{ NUM_UNITS }},
                "min_size": {{ NUM_THREADS }},
                ## Since all loads to/from global memory are across a wide bus, use that as the
                ## cache_width to optimize for contigous regions of DRAM for each inner block
                "cache_width": {{ CACHE_WIDTH }}
            }
        },
 
        ## We now move any temporaries even further down (into the per-output loop)
        { "name": "localize_main", "localize": { "reqs": ["main"] } },

        ## Scalarize, which should fully remove any remaining temporaries and convert them to scalars
        { "name": "scalarize_main", "scalarize": { "reqs": ["main"] } },

        ## We now relabel any remaining buffer inside the contractions as local memory
        { "name": "make_local", "locate_memory": { "reqs": ["contract_outer"], "loc": { "name": "LOCAL"} } },

        ## After all fusion, eliminate dead code again
        { "name": "dead_code_elimination", "dead_code_elimination": {"reqs": ["all"] } },

        ## Next, we thread all of the code that moves data between global and local memory so
        ## that multiple GPU threads can cooperatively do the work
        {
            "name": "thread_cache",
            "autotile": {
                "reqs": ["cache"],
                "outer_set": ["cache_outer", "gpu_thread"],
                "inner_set": ["cache_threads"],
                "only_po2": true,
                "max_sizes_product": {{ NUM_THREADS }},
                "cache_width": {{ CACHE_WIDTH }},
                "loc_name": "GLOBAL",
                "flip": true
            }
        },
  
        ## Finally we run the GPU specific thread_inner pass to thread the primary computation loop
        { "name": "thread_contract", "thread_inner" : { "reqs": ["contract_inner"], "threads": {{ NUM_THREADS }} } },
        {
            "name": "thread_eltwise",
            "autotile": {
                "reqs": ["eltwise_inner"],
                "outer_set": ["eltwise_struct", "gpu_thread"],
                "inner_set": ["eltwise_threads"],
                "only_po2": true,
                "max_sizes_product": {{ NUM_THREADS }},
                "cache_width": {{ CACHE_WIDTH }},
                "loc_name": "GLOBAL",
                "flip": true
            }
        },

        ## Thread leftover elementwise 
        { "name": "thread_elemwise", "thread_inner" : { "reqs": ["kernel", "eltwise"], "threads": {{ NUM_THREADS }} } },

        ## One last attempt to fuse across gpu_threads 
        ## { "name": "fuse_inner", "fusion": { "a_reqs": ["gpu_thread"], "b_reqs": ["gpu_thread"], "perfect": true } },

        ## Next, let's fuse any leftover elementwise ops at the main level
        ## { "name": "fuse_pure_elemwise", "fusion": { "parent_reqs": ["main"], "a_reqs": ["eltwise"], "b_reqs": ["eltwise"] } },

        ## We now locate the kernels onto the GPU compute device
        { "name": "loc_gpu", "locate_inner_block": { "reqs": ["kernel"], "loc": { "name": "GPU" } } },

        ## Final cleanup 
        { "name": "localize_main_2", "localize": { "reqs": ["main"] } },
        { "name": "scalarize_main_2", "scalarize": { "reqs": ["main"] } },
        { "name": "cleanup1", "prune_refs": { "reqs": ["main"] } },
        { "name": "cleanup2", "prune_idxs": { "reqs": ["main"] } },

        ## Compute the inter-kernel dependencies
        { "name": "compute_deps", "compute_deps": { } }, 

        ## Assign global memory addresses
        { "name": "place_program", "memory_placement": { "reqs": ["program"], "locs": [{ "name": "GLOBAL" }], "alignment": 4 } }
    ]
}
